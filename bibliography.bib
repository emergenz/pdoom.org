@article{radford2018improving,
  title = {Improving language understanding by generative pre-training},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and
            Sutskever, Ilya and others},
}

@article{radford2019language,
  title = {Language models are unsupervised multitask learners},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and
            Amodei, Dario and Sutskever, Ilya and others},
  journal = {OpenAI blog},
  volume = {1},
  number = {8},
  pages = {9},
  year = {2019},
}

@article{brown2020language,
  title = {Language models are few-shot learners},
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie
            and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind
            and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal = {Advances in neural information processing systems},
  volume = {33},
  pages = {1877--1901},
  year = {2020},
}

@article{raffel2020exploring,
  title = {Exploring the limits of transfer learning with a unified text-to-text
           transformer},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine
            and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei
            and Liu, Peter J},
  journal = {Journal of machine learning research},
  volume = {21},
  number = {140},
  pages = {1--67},
  year = {2020},
}

@article{touvron2023llama,
  title = {Llama 2: Open foundation and fine-tuned chat models},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter
            and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and
            Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal = {arXiv preprint arXiv:2307.09288},
  year = {2023},
}

@article{bai2023qwen,
  title = {Qwen technical report},
  author = {Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang,
            Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and
            Huang, Fei and others},
  journal = {arXiv preprint arXiv:2309.16609},
  year = {2023},
}

@article{young2024yi,
  title = {Yi: Open foundation models by 01. ai},
  author = {Young, Alex and Chen, Bei and Li, Chao and Huang, Chengen and Zhang,
            Ge and Zhang, Guanwei and Li, Heng and Zhu, Jiangcheng and Chen,
            Jianqun and Chang, Jing and others},
  journal = {arXiv preprint arXiv:2403.04652},
  year = {2024},
}

@article{vaswani2017attention,
  title = {Attention is all you need},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit,
            Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and
            Polosukhin, Illia},
  journal = {Advances in neural information processing systems},
  volume = {30},
  year = {2017},
}

@article{raffel2020exploring,
  title = {Exploring the limits of transfer learning with a unified text-to-text
           transformer},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine
            and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei
            and Liu, Peter J},
  journal = {Journal of machine learning research},
  volume = {21},
  number = {140},
  pages = {1--67},
  year = {2020},
}

@inproceedings{zhou2024what,
  title = {What Algorithms can Transformers Learn? A Study in Length
           Generalization},
  author = {Hattie Zhou and Arwen Bradley and Etai Littwin and Noam Razin and
            Omid Saremi and Joshua M. Susskind and Samy Bengio and Preetum
            Nakkiran},
  booktitle = {The Twelfth International Conference on Learning Representations},
  year = {2024},
  url = {https://openreview.net/forum?id=AssIuHnmHX},
}

@inproceedings{ding2024causallm,
  title = {Causal{LM} is not optimal for in-context learning},
  author = {Nan Ding and Tomer Levinboim and Jialin Wu and Sebastian Goodman and
            Radu Soricut},
  booktitle = {The Twelfth International Conference on Learning Representations},
  year = {2024},
  url = {https://openreview.net/forum?id=guRNebwZBb},
}

@article{williams1989learning,
  title = {A learning algorithm for continually running fully recurrent neural
           networks},
  author = {Williams, Ronald J and Zipser, David},
  journal = {Neural computation},
  volume = {1},
  number = {2},
  pages = {270--280},
  year = {1989},
  publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA
               journals-info~â€¦},
}

@article{tay2022ul2,
  title = {Ul2: Unifying language learning paradigms},
  author = {Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q and Garcia, Xavier
            and Wei, Jason and Wang, Xuezhi and Chung, Hyung Won and Shakeri,
            Siamak and Bahri, Dara and Schuster, Tal and others},
  journal = {arXiv preprint arXiv:2205.05131},
  year = {2022},
}

@misc{pfau2023last,
  title = {Last I checked, it was still not possible for a neural network alone
           (i.e. no MCTS) to beat the world's best Go players...},
  author = {Pfau, David},
  year = {2023},
  url = {https://twitter.com/pfau/status/1732785418565796167},
  note = {Accessed: 2023-12-07},
}

@article{deepmind2023alphacode,
  title = {AlphaCode 2 Technical Report},
  author = {Team, AlphaCode and Deepmind, Google},
  year = {2023},
  journal = {Google Deepmind},
  url = {
         https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf
         },
}

@article{reuters2023sam,
  author = {Tong, Anna and Dastin, Jeffrey and Hu, Krystal},
  title = {Sam Altman's ouster from OpenAI was precipitated by letter to board
           about AI breakthrough},
  journal = {Reuters},
  year = {2023},
  url = {
         https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/
         },
  note = {Accessed: 2023-12-07},
}

@misc{imbue2023podcast,
  title = {Noam Brown, FAIR: On achieving human-level performance in poker and
           Diplomacy, and the power of spending compute at inference time},
  author = {Noam Brown},
  howpublished = {
                  https://imbue.com/podcast/2023-02-09-podcast-episode-27-noam-brown/
                  },
  year = {2023},
  note = {Podcast episode 27, February 9, 2023},
}

@misc{karpathy2023youtube,
  author = {Karpathy, Andrej},
  title = {[1hr Talk] Intro to Large Language Models},
  howpublished = {YouTube},
  year = {2023},
  note = {Accessed: 2023-12-07},
  url = {https://www.youtube.com/watch?v=zjkBMFhNj_g&t=2100s},
}

@article{brown2019superhuman,
  title = {Superhuman AI for multiplayer poker},
  author = {Brown, Noam and Sandholm, Tuomas},
  journal = {Science},
  volume = {365},
  number = {6456},
  pages = {885--890},
  year = {2019},
  publisher = {American Association for the Advancement of Science},
}

@article{silver2016mastering,
  title = {Mastering the game of Go with deep neural networks and tree search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur
            and Sifre, Laurent and Van Den Driessche, George and Schrittwieser,
            Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot,
            Marc and others},
  journal = {nature},
  volume = {529},
  number = {7587},
  pages = {484--489},
  year = {2016},
  publisher = {Nature Publishing Group},
}

@article{schrittwieser2020mastering,
  title = {Mastering atari, go, chess and shogi by planning with a learned model
           },
  author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and
            Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez,
            Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore
            and others},
  journal = {Nature},
  volume = {588},
  number = {7839},
  pages = {604--609},
  year = {2020},
  publisher = {Nature Publishing Group UK London},
}

@article{wei2022chain,
  title = {Chain-of-thought prompting elicits reasoning in large language models
           },
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten
            and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal = {Advances in neural information processing systems},
  volume = {35},
  pages = {24824--24837},
  year = {2022},
}

@article{yao2024tree,
  title = {Tree of thoughts: Deliberate problem solving with large language
           models},
  author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and
            Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  year = {2024},
}

@article{lecun2022path,
  title = {A path towards autonomous machine intelligence version 0.9. 2,
           2022-06-27},
  author = {LeCun, Yann},
  journal = {Open Review},
  volume = {62},
  number = {1},
  year = {2022},
}

@article{hoffmann2022training,
  title = {Training compute-optimal large language models},
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and
            Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas,
            Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark,
            Aidan and others},
  journal = {arXiv preprint arXiv:2203.15556},
  year = {2022},
}

@article{meta2024introducing,
  title = {Introducing meta llama 3: The most capable openly available llm to
           date},
  author = {Meta, AI},
  journal = {Meta AI.},
  year = {2024},
}

@misc{riley2024it,
  title = {It's just not a very useful scaling law.},
  author = {@riley_stews},
  year = {2024},
  url = {https://x.com/riley_stews/status/1781019732122198288},
  note = {Accessed: 2023-04-20},
}

@article{shazeer2017outrageously,
  title = {Outrageously large neural networks: The sparsely-gated
           mixture-of-experts layer},
  author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and
            Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal = {arXiv preprint arXiv:1701.06538},
  year = {2017},
}

@article{fedus2022switch,
  title = {Switch transformers: Scaling to trillion parameter models with simple
           and efficient sparsity},
  author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal = {Journal of Machine Learning Research},
  volume = {23},
  number = {120},
  pages = {1--39},
  year = {2022},
}

@article{schulman2015high,
  title = {High-dimensional continuous control using generalized advantage
           estimation},
  author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan,
            Michael and Abbeel, Pieter},
  journal = {arXiv preprint arXiv:1506.02438},
  year = {2015},
}

@article{srambical2025ppo,
  author = {Srambical, Franz},
  title = {PPO Is Secretly Using Monte Carlo Advantage Estimation In LLM
           Post-Training},
  journal = {p(doom) blog},
  year = {2025},
  note = {https://pdoom.org/blog.html},
}

@article{williams1992simple,
  title = {Simple statistical gradient-following algorithms for connectionist
           reinforcement learning},
  author = {Williams, Ronald J},
  journal = {Machine learning},
  volume = {8},
  pages = {229--256},
  year = {1992},
  publisher = {Springer},
}

@software{deepmind2020jax,
  title = {The {D}eep{M}ind {JAX} {E}cosystem},
  author = {DeepMind and Babuschkin, Igor and Baumli, Kate and Bell, Alison and
            Bhupatiraju, Surya and Bruce, Jake and Buchlovsky, Peter and Budden,
            David and Cai, Trevor and Clark, Aidan and Danihelka, Ivo and Dedieu,
            Antoine and Fantacci, Claudio and Godwin, Jonathan and Jones, Chris
            and Hemsley, Ross and Hennigan, Tom and Hessel, Matteo and Hou,
            Shaobo and Kapturowski, Steven and Keck, Thomas and Kemaev, Iurii and
            King, Michael and Kunesch, Markus and Martens, Lena and Merzic, Hamza
            and Mikulik, Vladimir and Norman, Tamara and Papamakarios, George and
            Quan, John and Ring, Roman and Ruiz, Francisco and Sanchez, Alvaro
            and Sartran, Laurent and Schneider, Rosalia and Sezener, Eren and
            Spencer, Stephen and Srinivasan, Srivatsan and Stanojevi\'{c}, Milo\v
            {s} and Stokowiec, Wojciech and Wang, Luyu and Zhou, Guangyao and
            Viola, Fabio},
  url = {http://github.com/deepmind},
  year = {2020},
}

@misc{jax2025jit,
  title = {JAX: Just-in-time compilation},
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James
            Johnson and Chris Leary and Dougal Maclaurin and George Necula and
            Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao
            Zhang},
  year = {2025},
  url = {https://docs.jax.dev/en/latest/jit-compilation.html},
  note = {Accessed: 2025-03-26},
}

@misc{jax2025callbacks,
  title = {JAX: External callbacks},
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James
            Johnson and Chris Leary and Dougal Maclaurin and George Necula and
            Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao
            Zhang},
  year = {2025},
  url = {https://docs.jax.dev/en/latest/external-callbacks.html},
  note = {Accessed: 2025-03-26},
}

@misc{jax2025checkify,
  title = {JAX: The `checkify` transformation},
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James
            Johnson and Chris Leary and Dougal Maclaurin and George Necula and
            Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao
            Zhang},
  year = {2025},
  url = {https://docs.jax.dev/en/latest/debugging/checkify_guide.html},
  note = {Accessed: 2025-03-26},
}

@misc{jax2025key,
  title = {JAX: Key concepts},
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James
            Johnson and Chris Leary and Dougal Maclaurin and George Necula and
            Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao
            Zhang},
  year = {2025},
  url = {https://docs.jax.dev/en/latest/key-concepts.html},
  note = {Accessed: 2025-03-26},
}

@software{deepmind2020chex,
  title = {Chex},
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James
            Johnson and Chris Leary and Dougal Maclaurin and George Necula and
            Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao
            Zhang},
  url = {http://github.com/google-deepmind/chex},
  year = {2020},
}

@misc{jax2025control,
  title = {JAX: Control flow and logical operators with JIT},
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James
            Johnson and Chris Leary and Dougal Maclaurin and George Necula and
            Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao
            Zhang},
  year = {2025},
  url = {https://docs.jax.dev/en/latest/control-flow.html},
  note = {Accessed: 2025-03-26},
}

@misc{xla2025conditional,
  title = {XLA:Operation Semantics:Conditional},
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James
            Johnson and Chris Leary and Dougal Maclaurin and George Necula and
            Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao
            Zhang},
  year = {2025},
  url = {https://openxla.org/xla/operation_semantics#conditional},
  note = {Accessed: 2025-03-26},
}

@misc{ayaka76822025error,
  author = {ayaka7682},
  title = {Message on public Discord server: Try this:
  <d-code block="", language="python">
  import jax from jax._src.error_check import set_error_if, raise_if_errorn
  </d-code>
  <d-code block="", language="python">
  import jax.numpy as jnp
  </d-code>
  <d-code block="", language="python">
  @jax.jit
  </d-code>
  <d-code block="", language="python">
  def f(x, y):
  </d-code>
  <d-code block="", language="python">
    set_error_if(x != 0, 'x must be 0')
  </d-code>
  <d-code block="", language="python">
    return jnp.multiply(x, y)
  </d-code>
  <d-code block="", language="python">
  f(0, 0)
  </d-code>
  <d-code block="", language="python">
  raise_if_error()
  </d-code> },
  year = {2025},
  url = {
         https://discord.com/channels/1107832795377713302/1107832795688083561/1354171414596419854
         },
  note = {Accessed: 2025-03-26},
}
