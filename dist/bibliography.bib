@article{radford2018improving,
  title = {Improving language understanding by generative pre-training},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and
            Sutskever, Ilya and others},
}

@article{radford2019language,
  title = {Language models are unsupervised multitask learners},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and
            Amodei, Dario and Sutskever, Ilya and others},
  journal = {OpenAI blog},
  volume = {1},
  number = {8},
  pages = {9},
  year = {2019},
}

@article{brown2020language,
  title = {Language models are few-shot learners},
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie
            and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind
            and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal = {Advances in neural information processing systems},
  volume = {33},
  pages = {1877--1901},
  year = {2020},
}

@article{raffel2020exploring,
  title = {Exploring the limits of transfer learning with a unified text-to-text
           transformer},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine
            and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei
            and Liu, Peter J},
  journal = {Journal of machine learning research},
  volume = {21},
  number = {140},
  pages = {1--67},
  year = {2020},
}

@article{touvron2023llama,
  title = {Llama 2: Open foundation and fine-tuned chat models},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter
            and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and
            Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal = {arXiv preprint arXiv:2307.09288},
  year = {2023},
}

@article{bai2023qwen,
  title = {Qwen technical report},
  author = {Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang,
            Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and
            Huang, Fei and others},
  journal = {arXiv preprint arXiv:2309.16609},
  year = {2023},
}

@article{young2024yi,
  title = {Yi: Open foundation models by 01. ai},
  author = {Young, Alex and Chen, Bei and Li, Chao and Huang, Chengen and Zhang,
            Ge and Zhang, Guanwei and Li, Heng and Zhu, Jiangcheng and Chen,
            Jianqun and Chang, Jing and others},
  journal = {arXiv preprint arXiv:2403.04652},
  year = {2024},
}

@article{vaswani2017attention,
  title = {Attention is all you need},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit,
            Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and
            Polosukhin, Illia},
  journal = {Advances in neural information processing systems},
  volume = {30},
  year = {2017},
}

@article{raffel2020exploring,
  title = {Exploring the limits of transfer learning with a unified text-to-text
           transformer},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine
            and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei
            and Liu, Peter J},
  journal = {Journal of machine learning research},
  volume = {21},
  number = {140},
  pages = {1--67},
  year = {2020},
}

@inproceedings{zhou2024what,
  title = {What Algorithms can Transformers Learn? A Study in Length
           Generalization},
  author = {Hattie Zhou and Arwen Bradley and Etai Littwin and Noam Razin and
            Omid Saremi and Joshua M. Susskind and Samy Bengio and Preetum
            Nakkiran},
  booktitle = {The Twelfth International Conference on Learning Representations},
  year = {2024},
  url = {https://openreview.net/forum?id=AssIuHnmHX},
}

@inproceedings{ding2024causallm,
  title = {Causal{LM} is not optimal for in-context learning},
  author = {Nan Ding and Tomer Levinboim and Jialin Wu and Sebastian Goodman and
            Radu Soricut},
  booktitle = {The Twelfth International Conference on Learning Representations},
  year = {2024},
  url = {https://openreview.net/forum?id=guRNebwZBb},
}

@article{williams1989learning,
  title = {A learning algorithm for continually running fully recurrent neural
           networks},
  author = {Williams, Ronald J and Zipser, David},
  journal = {Neural computation},
  volume = {1},
  number = {2},
  pages = {270--280},
  year = {1989},
  publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA
               journals-info~â€¦},
}

@article{tay2022ul2,
  title = {Ul2: Unifying language learning paradigms},
  author = {Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q and Garcia, Xavier
            and Wei, Jason and Wang, Xuezhi and Chung, Hyung Won and Shakeri,
            Siamak and Bahri, Dara and Schuster, Tal and others},
  journal = {arXiv preprint arXiv:2205.05131},
  year = {2022},
}

@misc{pfau2023last,
  title = {Last I checked, it was still not possible for a neural network alone
           (i.e. no MCTS) to beat the world's best Go players...},
  author = {Pfau, David},
  year = {2023},
  url = {https://twitter.com/pfau/status/1732785418565796167},
  note = {Accessed: 2023-12-07},
}

@article{deepmind2023alphacode,
  title = {AlphaCode 2 Technical Report},
  author = {Team, AlphaCode and Deepmind, Google},
  year = {2023},
  journal = {Google Deepmind},
  url = {
         https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf
         },
}

@article{reuters2023sam,
  author = {Tong, Anna and Dastin, Jeffrey and Hu, Krystal},
  title = {Sam Altman's ouster from OpenAI was precipitated by letter to board
           about AI breakthrough},
  journal = {Reuters},
  year = {2023},
  url = {
         https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/
         },
  note = {Accessed: 2023-12-07},
}

@misc{imbue2023podcast,
  title = {Noam Brown, FAIR: On achieving human-level performance in poker and
           Diplomacy, and the power of spending compute at inference time},
  author = {Noam Brown},
  howpublished = {
                  https://imbue.com/podcast/2023-02-09-podcast-episode-27-noam-brown/
                  },
  year = {2023},
  note = {Podcast episode 27, February 9, 2023},
}

@misc{karpathy2023youtube,
  author = {Karpathy, Andrej},
  title = {[1hr Talk] Intro to Large Language Models},
  howpublished = {YouTube},
  year = {2023},
  note = {Accessed: 2023-12-07},
  url = {https://www.youtube.com/watch?v=zjkBMFhNj_g&t=2100s},
}

@article{brown2019superhuman,
  title = {Superhuman AI for multiplayer poker},
  author = {Brown, Noam and Sandholm, Tuomas},
  journal = {Science},
  volume = {365},
  number = {6456},
  pages = {885--890},
  year = {2019},
  publisher = {American Association for the Advancement of Science},
}

@article{silver2016mastering,
  title = {Mastering the game of Go with deep neural networks and tree search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur
            and Sifre, Laurent and Van Den Driessche, George and Schrittwieser,
            Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot,
            Marc and others},
  journal = {nature},
  volume = {529},
  number = {7587},
  pages = {484--489},
  year = {2016},
  publisher = {Nature Publishing Group},
}

@article{schrittwieser2020mastering,
  title = {Mastering atari, go, chess and shogi by planning with a learned model
           },
  author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and
            Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez,
            Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore
            and others},
  journal = {Nature},
  volume = {588},
  number = {7839},
  pages = {604--609},
  year = {2020},
  publisher = {Nature Publishing Group UK London},
}

@article{wei2022chain,
  title = {Chain-of-thought prompting elicits reasoning in large language models
           },
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten
            and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal = {Advances in neural information processing systems},
  volume = {35},
  pages = {24824--24837},
  year = {2022},
}

@article{yao2024tree,
  title = {Tree of thoughts: Deliberate problem solving with large language
           models},
  author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and
            Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  year = {2024},
}

@article{lecun2022path,
  title = {A path towards autonomous machine intelligence version 0.9. 2,
           2022-06-27},
  author = {LeCun, Yann},
  journal = {Open Review},
  volume = {62},
  number = {1},
  year = {2022},
}

@article{hoffmann2022training,
  title = {Training compute-optimal large language models},
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and
            Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas,
            Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark,
            Aidan and others},
  journal = {arXiv preprint arXiv:2203.15556},
  year = {2022},
}

@article{meta2024introducing,
  title = {Introducing meta llama 3: The most capable openly available llm to
           date},
  author = {Meta, AI},
  journal = {Meta AI.},
  year = {2024},
}

@misc{riley2024it,
  title = {It's just not a very useful scaling law.},
  author = {@riley_stews},
  year = {2024},
  url = {https://x.com/riley_stews/status/1781019732122198288},
  note = {Accessed: 2023-04-20},
}

@article{shazeer2017outrageously,
  title = {Outrageously large neural networks: The sparsely-gated
           mixture-of-experts layer},
  author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and
            Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal = {arXiv preprint arXiv:1701.06538},
  year = {2017},
}

@article{fedus2022switch,
  title = {Switch transformers: Scaling to trillion parameter models with simple
           and efficient sparsity},
  author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal = {Journal of Machine Learning Research},
  volume = {23},
  number = {120},
  pages = {1--39},
  year = {2022},
}

@article{schulman2015high,
  title = {High-dimensional continuous control using generalized advantage
           estimation},
  author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan,
            Michael and Abbeel, Pieter},
  journal = {arXiv preprint arXiv:1506.02438},
  year = {2015},
}

@article{srambical2025ppo,
  author = {Srambical, Franz},
  title = {PPO Is Secretly Using Monte Carlo Advantage Estimation In LLM
           Post-Training},
  journal = {p(doom) blog},
  year = {2025},
  note = {https://pdoom.org/blog.html},
}

@article{williams1992simple,
  title = {Simple statistical gradient-following algorithms for connectionist
           reinforcement learning},
  author = {Williams, Ronald J},
  journal = {Machine learning},
  volume = {8},
  pages = {229--256},
  year = {1992},
  publisher = {Springer},
}

@software{deepmind2020jax,
  title = {The {D}eep{M}ind {JAX} {E}cosystem},
  author = {DeepMind and Babuschkin, Igor and Baumli, Kate and Bell, Alison and
            Bhupatiraju, Surya and Bruce, Jake and Buchlovsky, Peter and Budden,
            David and Cai, Trevor and Clark, Aidan and Danihelka, Ivo and Dedieu,
            Antoine and Fantacci, Claudio and Godwin, Jonathan and Jones, Chris
            and Hemsley, Ross and Hennigan, Tom and Hessel, Matteo and Hou,
            Shaobo and Kapturowski, Steven and Keck, Thomas and Kemaev, Iurii and
            King, Michael and Kunesch, Markus and Martens, Lena and Merzic, Hamza
            and Mikulik, Vladimir and Norman, Tamara and Papamakarios, George and
            Quan, John and Ring, Roman and Ruiz, Francisco and Sanchez, Alvaro
            and Sartran, Laurent and Schneider, Rosalia and Sezener, Eren and
            Spencer, Stephen and Srinivasan, Srivatsan and Stanojevi\'{c}, Milo\v
            {s} and Stokowiec, Wojciech and Wang, Luyu and Zhou, Guangyao and
            Viola, Fabio},
  url = {http://github.com/deepmind},
  year = {2020},
}

@misc{jax2025jit,
  title = {JAX: Just-in-time compilation},
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James
            Johnson and Chris Leary and Dougal Maclaurin and George Necula and
            Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao
            Zhang},
  year = {2025},
  url = {https://docs.jax.dev/en/latest/jit-compilation.html},
  note = {Accessed: 2025-03-26},
}

@misc{jax2025callbacks,
  title = {JAX: External callbacks},
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James
            Johnson and Chris Leary and Dougal Maclaurin and George Necula and
            Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao
            Zhang},
  year = {2025},
  url = {https://docs.jax.dev/en/latest/external-callbacks.html},
  note = {Accessed: 2025-03-26},
}

@misc{jax2025checkify,
  title = {JAX: The `checkify` transformation},
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James
            Johnson and Chris Leary and Dougal Maclaurin and George Necula and
            Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao
            Zhang},
  year = {2025},
  url = {https://docs.jax.dev/en/latest/debugging/checkify_guide.html},
  note = {Accessed: 2025-03-26},
}

@misc{jax2025key,
  title = {JAX: Key concepts},
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James
            Johnson and Chris Leary and Dougal Maclaurin and George Necula and
            Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao
            Zhang},
  year = {2025},
  url = {https://docs.jax.dev/en/latest/key-concepts.html},
  note = {Accessed: 2025-03-26},
}

@software{deepmind2020chex,
  title = {Chex},
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James
            Johnson and Chris Leary and Dougal Maclaurin and George Necula and
            Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao
            Zhang},
  url = {http://github.com/google-deepmind/chex},
  year = {2020},
}

@misc{jax2025control,
  title = {JAX: Control flow and logical operators with JIT},
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James
            Johnson and Chris Leary and Dougal Maclaurin and George Necula and
            Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao
            Zhang},
  year = {2025},
  url = {https://docs.jax.dev/en/latest/control-flow.html},
  note = {Accessed: 2025-03-26},
}

@misc{xla2025conditional,
  title = {XLA:Operation Semantics:Conditional},
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James
            Johnson and Chris Leary and Dougal Maclaurin and George Necula and
            Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao
            Zhang},
  year = {2025},
  url = {https://openxla.org/xla/operation_semantics#conditional},
  note = {Accessed: 2025-03-26},
}

@misc{ayaka76822025error,
  author = {ayaka7682},
  title = {Message on public Discord server: Try this:
  <d-code block="", language="python">
  import jax from jax._src.error_check import set_error_if, raise_if_error
  </d-code>
  <d-code block="", language="python">
  import jax.numpy as jnp
  </d-code>
  <d-code block="", language="python">
  @jax.jit
  </d-code>
  <d-code block="", language="python">
  def f(x, y):
  </d-code>
  <d-code block="", language="python">
    set_error_if(x != 0, 'x must be 0')
  </d-code>
  <d-code block="", language="python">
    return jnp.multiply(x, y)
  </d-code>
  <d-code block="", language="python">
  f(0, 0)
  </d-code>
  <d-code block="", language="python">
  raise_if_error()
  </d-code> },
  year = {2025},
  url = {
         https://discord.com/channels/1107832795377713302/1107832795688083561/1354171414596419854
         },
  note = {Accessed: 2025-03-26},
}

@book{sutton1998reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G and others},
  volume={1},
  number={1},
  year={1998},
  publisher={MIT press Cambridge}
}

@article{sutton1999policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}

@article{degris2012off,
  title={Off-policy actor-critic},
  author={Degris, Thomas and White, Martha and Sutton, Richard S},
  journal={arXiv preprint arXiv:1205.4839},
  year={2012}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}


@misc{openai2025imo,
  title = {We achieved gold medal-level performance ðŸ¥‡on the 2025 International Mathematical Olympiad with a general-purpose reasoning LLM!},
  author = {OpenAI},
  year = {2025},
  url = {https://x.com/OpenAI/status/1946594928945148246},
  note = {Accessed: 2025-08-05},
}

@misc{deepmind2025imo,
  title = {Advanced version of Gemini with Deep Think officially achieves gold-medal standard at the International Mathematical Olympiad},
  author = {Luong, Thang and Lockhart, Edward},
  year = {2025},
  url = {https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/},
  note = {DeepMind Blog, July 21, 2025},
}

@misc{deepseekai2025r1,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}

@misc{cursor2025tab,
  title = {A New Tab Model},
  author = {Cursor},
  year = {2025},
  url = {https://cursor.com/blog/tab-update},
  note = {Accessed: 2025-08-05},
}

@inproceedings{bruce2024genie,
    title={Genie: Generative Interactive Environments},
    author={Jake Bruce and Michael D Dennis and Ashley Edwards and Jack Parker-Holder and Yuge Shi and Edward Hughes and Matthew Lai and Aditi Mavalankar and Richie Steigerwald and Chris Apps and Yusuf Aytar and Sarah Maria Elisabeth Bechtle and Feryal Behbahani and Stephanie C.Y. Chan and Nicolas Heess and Lucy Gonzalez and Simon Osindero and Sherjil Ozair and Scott Reed and Jingwei Zhang and Konrad Zolna and Jeff Clune and Nando de Freitas and Satinder Singh and Tim Rockt{\"a}schel},
    booktitle={Forty-first International Conference on Machine Learning},
    year={2024},
    url={https://openreview.net/forum?id=bJbSbJskOS}
}

@article{parkerholder2024genie2,
  title         = {Genie 2: A Large-Scale Foundation World Model},
  author        = {Jack Parker-Holder and Philip Ball and Jake Bruce and Vibhavari Dasagi and Kristian Holsheimer and Christos Kaplanis and Alexandre Moufarek and Guy Scully and Jeremy Shar and Jimmy Shi and Stephen Spencer and Jessica Yung and Michael Dennis and Sultan Kenjeyev and Shangbang Long and Vlad Mnih and Harris Chan and Maxime Gazeau and Bonnie Li and Fabio Pardo and Luyu Wang and Lei Zhang and Frederic Besse and Tim Harley and Anna Mitenkova and Jane Wang and Jeff Clune and Demis Hassabis and Raia Hadsell and Adrian Bolton and Satinder Singh and Tim Rockt{\"a}schel},
  year          = {2024},
  url           = {https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/}
}

@article{deepmind2025genie3,
  title         = {Genie 3: A New Frontier for World Models},
  author        = {Philip J. Ball and Jakob Bauer and Frank Belletti and Bethanie Brownfield and Ariel Ephrat and Shlomi Fruchter and Agrim Gupta and Kristian Holsheimer and Aleksander Holynski and Jiri Hron and Christos Kaplanis and Marjorie Limont and Matt McGill and Yanko Oliveira and Jack Parker-Holder and Frank Perbet and Guy Scully and Jeremy Shar and Stephen Spencer and Omer Tov and Ruben Villegas and Emma Wang and Jessica Yung and Cip Baetu and Jordi Berbel and David Bridson and Jake Bruce and Gavin Buttimore and Sarah Chakera and Bilva Chandra and Paul Collins and Alex Cullum and Bogdan Damoc and Vibha Dasagi and Maxime Gazeau and Charles Gbadamosi and Woohyun Han and Ed Hirst and Ashyana Kachra and Lucie Kerley and Kristian Kjems and Eva Knoepfel and Vika Koriakin and Jessica Lo and Cong Lu and Zeb Mehring and Alex Moufarek and Henna Nandwani and Valeria Oliveira and Fabio Pardo and Jane Park and Andrew Pierson and Ben Poole and Helen Ran and Tim Salimans and Manuel Sanchez and Igor Saprykin and Amy Shen and Sailesh Sidhwani and Duncan Smith and Joe Stanton and Hamish Tomlinson and Dimple Vijaykumar and Luyu Wang and Piers Wingfield and Nat Wong and Keyang Xu and Christopher Yew and Nick Young and Vadim Zubov and Douglas Eck and Dumitru Erhan and Koray Kavukcuoglu and Demis Hassabis and Zoubin Gharamani and Raia Hadsell and A{\"a}ron van den Oord and Inbar Mosseri and Adrian Bolton and Satinder Singh and Tim Rockt{\"a}schel},
  year          = {2025},
  url           = {}
}

@InProceedings{parkerholder2022evolving,
  title = 	 {Evolving Curricula with Regret-Based Environment Design},
  author =       {Parker-Holder, Jack and Jiang, Minqi and Dennis, Michael and Samvelyan, Mikayel and Foerster, Jakob and Grefenstette, Edward and Rockt{\"a}schel, Tim},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {17473--17498},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/parker-holder22a/parker-holder22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/parker-holder22a.html},
  abstract = 	 {Training generally-capable agents with reinforcement learning (RL) remains a significant challenge. A promising avenue for improving the robustness of RL agents is through the use of curricula. One such class of methods frames environment design as a game between a student and a teacher, using regret-based objectives to produce environment instantiations (or levels) at the frontier of the student agentâ€™s capabilities. These methods benefit from theoretical robustness guarantees at equilibrium, yet they often struggle to find effective levels in challenging design spaces in practice. By contrast, evolutionary approaches incrementally alter environment complexity, resulting in potentially open-ended learning, but often rely on domain-specific heuristics and vast amounts of computational resources. This work proposes harnessing the power of evolution in a principled, regret-based curriculum. Our approach, which we call Adversarially Compounding Complexity by Editing Levels (ACCEL), seeks to constantly produce levels at the frontier of an agentâ€™s capabilities, resulting in curricula that start simple but become increasingly complex. ACCEL maintains the theoretical benefits of prior regret-based methods, while providing significant empirical gains in a diverse set of environments. An interactive version of this paper is available at https://accelagent.github.io.}
}

@article{agarwal2025cosmos,
  title = {Cosmos World Foundation Model Platform for Physical AI},
  author = {Agarwal, Niket and others},
  journal = {arXiv preprint arXiv:2501.03575},
  year = {2025}
}

@article{bellemare2013arcade,
  title = {The arcade learning environment: An evaluation platform for general agents},
  author = {Bellemare, Marc G and others},
  journal = {Journal of artificial intelligence research},
  volume = {47},
  pages = {253--279},
  year = {2013}
}

@article{nichol2018retro,
  title={Gotta Learn Fast: A New Benchmark for Generalization in RL},
  author={Nichol, Alex and Pfau, Vicki and Hesse, Christopher and Klimov, Oleg and Schulman, John},
  journal={arXiv preprint arXiv:1804.03720},
  year={2018}
}

@inproceedings{matthews2024craftax,
    author={Michael Matthews and Michael Beukman and Benjamin Ellis and Mikayel Samvelyan and Matthew Jackson and Samuel Coward and Jakob Foerster},
    title = {Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning},
    booktitle = {International Conference on Machine Learning ({ICML})},
    year = {2024}
}

@inproceedings{NEURIPS2022_9c7008af,
 author = {Baker, Bowen and Akkaya, Ilge and Zhokov, Peter and Huizinga, Joost and Tang, Jie and Ecoffet, Adrien and Houghton, Brandon and Sampedro, Raul  and Clune, Jeff},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {24639--24654},
 publisher = {Curran Associates, Inc.},
 title = {Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/9c7008aff45b5d8f0973b23e1a22ada0-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@inproceedings{osband2020bsuite,
    title={Behaviour Suite for Reinforcement Learning},
    author={Osband, Ian and
            Doron, Yotam and
            Hessel, Matteo and
            Aslanides, John and
            Sezener, Eren and
            Saraiva, Andre and
            McKinney, Katrina and
            Lattimore, Tor and
            {Sz}epesv{\'a}ri, Csaba and
            Singh, Satinder and
            Van Roy, Benjamin and
            Sutton, Richard and
            Silver, David and
            van Hasselt, Hado},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=rygf-kSYwH}
}

@article{nguyen2025crowd-sourcing,
  author = {Nguyen, Alfred and Mahajan, Mihir and Srambical, Franz},
  title = {Crowd-Sourcing A Dataset To Make Agents Code Like Humans},
  journal = {p(doom) blog},
  year = {2025},
  note = {https://pdoom.org/blog.html}
}