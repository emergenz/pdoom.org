<!--
  Copyright 2018 The Distill Template Authors

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!doctype html>

<head>
  <script src="template.v2.js"></script>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf8">
  <link rel="icon" type="image/png" href="favicon.png">
</head>

<body>
  <!--
  <distill-header></distill-header>
  -->
  <d-front-matter>
    <script id='distill-front-matter' type="text/json">{
    "title": "PPO On Verification Values All Tokens Equally",
    "description": "When using PPO on verification signals, PPO's value function learns to assign the same value to all tokens in a sequence: the sequence-level probability of successful verification.",
    "published": "February 12, 2025",
    "url": "https://pdoom.org/ppo.html",
    "authors": [
      {
        "author":"Franz Srambical",
        "authorURL":"https://srambical.fr/",
        "affiliations": [{"name": "p(doom)", "url": "https://pdoom.org/"}]
      }
    ],
    "katex": {
      "delimiters": [
        {"left": "$$", "right": "$$", "display": false}
      ]
    }
  }</script>
  </d-front-matter>
  <d-title>
    <p>
        When using PPO on verification signals, PPO's value function learns to assign the same value to all tokens in a sequence: the sequence-level probability of successful verification.
    </p>
  </d-title>
  <d-byline></d-byline>
  <d-article>
    <a class="marker" href="#section-1" id="section-1"><span>1</span></a>
    <p>
      The GAE paper <d-cite key="schulman2015high"></d-cite> defines generalized advantage as
      $$\hat{A}_t^{GAE(\gamma,\lambda)} := \sum_{l=0}^{\infty}(\gamma \lambda)^l \delta^V_{t+l}$$, which, in the LLM
      post-training regime, reduces to $$\hat{A}_t := r_{T} - V(s_t)$$ <d-cite key="srambical2025ppo"></d-cite>.
    </p>

    <p>
      The value function is usually learnt using a regression loss: $$\mathcal{L} = \sum_{t=1}^T ||V_\phi(s_t) - \sum_{l=0}^{\infty} r_{t+l}||^2$$,
      which in our setting reduces to $$\mathcal{L} = \sum_{t=1}^T ||V_\phi(s_t) - r_N||^2$$, hence the learnt value function is .

    </p>
  </d-article>

  <d-appendix>

    <h3>Contributions</h3>
    <p>FS worked on all aspects of this post, including research, analysis and writing.</p>
    <d-bibliography src="bibliography.bib"></d-bibliography>
    <distill-appendix>
    </distill-appendix>
  </d-appendix>

  <distill-footer></distill-footer>

</body>
